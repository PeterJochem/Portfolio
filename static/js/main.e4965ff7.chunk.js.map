{"version":3,"sources":["DescriptionPages/ROS_Navigation_Stack_Description.js","portfolio.js","DescriptionPages/Deep_RL_Description.js","App.js","index.js"],"names":["ROS_Navigation_Stack_Description","props","state","componentDidMount","bind","document","body","style","backgroundImage","className","to","id","href","class","src","alt","React","Component","GridOfPosts","onMouseEnterHandler","onMouseExitHandler","projectName","hover_url","getElementById","static_url","this","pageNumber","projects","map","project","name","url","onMouseEnter","onMouseLeave","im_url","defaultProps","Post","i","Portfolio","Deep_RL_Description","controls","App","results","page1","github_url","page2","Site","title","exact","path","render","ReactDOM"],"mappings":"iVAWqBA,G,wDACd,WAAYC,GAAQ,IAAD,8BACjB,cAAMA,IASdC,MAAQ,GARO,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFf,E,gEAOxBC,SAASC,KAAKC,MAAMC,gBAAkB,O,+BAKlC,OACE,yBAAKC,UAAY,uBAET,yBAAKA,UAAY,UACP,yBAAKA,UAAY,cAAjB,iBAGjB,yBAAKA,UAAY,YACD,kBAAC,IAAD,CAAMA,UAAY,eAAeC,GAAI,eAArC,UAIA,kBAAC,IAAD,CAAMD,UAAY,mBAAmBC,GAAI,cAAzC,mBAYlB,yBAAKC,GAAG,iBAET,4DAEC,iEACA,4vBAEA,isBAIA,gEACO,kCACI,uBAAGF,UAAU,iBAAiBG,KAAM,0BAApC,uBADJ,4oBAIP,yBAAKC,MAAM,OACF,yBAAKA,MAAM,4BACH,qCAAKC,IAAI,iHAAiHC,IAAI,6CAA9H,MAA8K,gDAKxL,gjBAAqhB,uBAAGN,UAAU,WAAWG,KAAM,qEAA9B,kCAArhB,yOAEb,m1BAKA,yBAAKC,MAAM,OACH,yBAAKA,MAAM,UACH,yBAAKC,IAAI,kKAAkKC,IAAI,8CASzL,iEACA,y2BAA80B,uBAAGN,UAAU,aAAaG,KAAK,mCAA/B,cAA90B,gZAA0yC,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCAA1yC,QAA05C,uBAAGH,UAAU,aAAaG,KAAK,wCAA/B,SAA15C,2PAGA,2BACL,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCADK,2OAC8U,uBAAGH,UAAU,aAAaG,KAAK,4CAA/B,gBAD9U,KAC0a,uBAAGH,UAAU,aAAaG,KAAK,kDAA/B,oBAD1a,SACohB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,2BADphB,+qBAIP,yBAAKC,MAAM,OACT,yBAAKA,MAAM,UACR,yBAAKC,IAAI,oGAAoGC,IAAI,uCAErH,yBAAKF,MAAM,UACR,yBAAKC,IAAI,uFAAuFC,IAAI,6BAA6BN,UAAU,8BAIxI,yBAAKK,IAAI,4FAA4FC,IAAI,uBAAuBN,UAAU,wB,GA/FnFO,IAAMC,YCJ9DC,E,kDACL,WAAYjB,GAAQ,IAAD,8BACJ,cAAMA,IACDE,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBACzB,EAAKe,oBAAsB,EAAKA,oBAAoBf,KAAzB,gBAClC,EAAKgB,mBAAqB,EAAKA,mBAAmBhB,KAAxB,gBAJf,E,6GASCiB,EAAaC,GACFjB,SAASkB,eAAeF,GAC9Bd,MAAMC,gBAAkB,OAASc,EAAY,M,yCAGnDD,EAAaG,GACfnB,SAASkB,eAAeF,GAChBd,MAAMC,gBAAkB,OAASgB,EAAa,M,+BAI1D,IAAD,OACP,OAEC,yBAAKf,UAAY,eACV,yBAAKA,UAAU,kBAErBgB,KAAKxB,MAAMyB,WAAWC,SAASC,KAAK,SAACC,GACrB,OAAQ,yBAAKpB,UAAY,OAEtCoB,EAAQC,KACf,uBAAGlB,KAAQiB,EAAQE,IAAKC,aAAc,kBAAM,EAAKb,oBAAoBU,EAAQC,KAAMD,EAAQP,YAClFW,aAAc,kBAAM,EAAKb,mBAAmBS,EAAQC,KAAMD,EAAQK,UAC5D,yBAAKzB,UAAU,YAAYE,GAAMkB,EAAQC,MAAzC,KAAiD,kBAAC,EAAD,CAAMA,KAAMD,EAAQC,KAAMI,OAAUL,EAAQK,OAAQZ,UAAaO,EAAQP,YAA1H,gB,GAjCON,IAAMC,WAiDhCC,EAAYiB,aAAe,CAC1BT,WAAY,M,IAIPU,E,kDACL,WAAYnC,GAAQ,IAAD,8BACf,cAAMA,IAIVC,MAAQ,CAACmC,EAAG,GAHX,EAAKlC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFP,E,gEAQFC,SAASkB,eAAeE,KAAKxB,MAAM6B,MACzCvB,MAAMC,gBAAkB,OAASiB,KAAKxB,MAAMiC,OAAS,M,+BAK3D,OACE,yBAAKzB,UAAY,a,GAhBNO,IAAMC,WA8BzBmB,EAAKD,aAAe,CACnBL,KAAM,KACNI,OAAQ,I,IAGYI,E,kDACd,WAAYrC,GAAQ,IAAD,8BACjB,cAAMA,IAQdC,MAAQ,GAPO,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFf,E,gEAMxBC,SAASC,KAAKC,MAAMC,gBAAkB,O,+BAKlC,OAEC,6BACC,yBAAKC,UAAY,aAChB,kBAAC,EAAD,CAAaiB,WAAYD,KAAKxB,MAAMyB,mB,GAhBNV,IAAMC,WA4B7CqB,EAAUH,aAAe,G,ICjHJI,E,kDACd,WAAYtC,GAAQ,IAAD,8BACjB,cAAMA,IAQdC,MAAQ,GAPO,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFf,E,gEAMxBC,SAASC,KAAKC,MAAMC,gBAAkB,O,+BAMlC,OACE,yBAAKC,UAAY,uBAET,yBAAKA,UAAY,UACP,yBAAKA,UAAY,cAAjB,iBAGjB,yBAAKA,UAAY,YACtB,uBAAGA,UAAY,eAAeG,KAAK,2HAAnC,UAIqB,kBAAC,IAAD,CAAMH,UAAY,mBAAmBC,GAAI,cAAzC,mBAMnB,yBAAKD,UAAU,wBACd,kBAAC,IAAD,CAAasB,IAAI,+DAA+DS,SAAS,OAAO/B,UAAU,iBAG3G,yBAAKE,GAAG,iBAEP,4EACA,yEAA8C,uBAAGC,KAAK,4EAA4EH,UAAU,cAA9F,cAA9C,uuBAA+5B,uBAAGG,KAAK,yCAAyCH,UAAU,cAA3D,gBAA/5B,KAEA,0QAEkP,uBAAGA,UAAU,aAAaG,KAAK,iDAA/B,iDAFlP,+EAEwc,uBAAGH,UAAU,aAAaG,KAAK,kDAA/B,oBAFxc,SAEkjB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,2BAFljB,wGAEovB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCAFpvB,QAEo2B,uBAAGH,UAAU,aAAaG,KAAK,wCAA/B,SAFp2B,2DAE2+B,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,iDAF3+B,gDAKA,gEACO,kCACI,uBAAGH,UAAU,iBAAiBG,KAAM,0BAApC,uBADJ,0oBAIP,yBAAKC,MAAM,OACF,yBAAKA,MAAM,4BACH,qCAAKC,IAAI,iHAAiHC,IAAI,6CAA9H,MAA8K,gDAKxL,ijBAAgiB,uBAAGN,UAAU,WAAWG,KAAM,qEAA9B,kCAAhiB,0OAEb,yzBAKA,yBAAKC,MAAM,OACH,yBAAKA,MAAM,UACH,yBAAKC,IAAI,kKAAkKC,IAAI,8CAIhM,iEAEO,01BAAy0B,uBAAGN,UAAU,aAAaG,KAAK,mCAA/B,cAAz0B,2fAGN,6VACoV,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCADpV,QACoc,uBAAGH,UAAU,aAAaG,KAAK,wCAA/B,SADpc,gQAIM,2BACL,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCADK,2OAC8U,uBAAGH,UAAU,aAAaG,KAAK,4CAA/B,gBAD9U,KAC0a,uBAAGH,UAAU,aAAaG,KAAK,kDAA/B,oBAD1a,SACohB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,2BADphB,grBAIP,yBAAKC,MAAM,OACT,yBAAKA,MAAM,UACR,yBAAKC,IAAI,oGAAoGC,IAAI,uCAErH,yBAAKF,MAAM,UACR,yBAAKC,IAAI,uFAAuFC,IAAI,6BAA6BN,UAAU,8BAI/I,uBAAGA,UAAU,4BAAb,6nBAKA,yBAAKI,MAAM,OACH,yBAAKA,MAAM,UACH,yBAAKC,IAAI,2FAA2FC,IAAI,uCAEhH,yBAAKF,MAAM,UACH,yBAAKC,IAAI,uHAAuHC,IAAI,6BAA6BN,UAAU,8BAI3L,uBAAGA,UAAU,4BAAb,+BAC6B,uBAAGA,UAAU,aAAaG,KAAK,oCAA/B,+CAD7B,qjC,GA1GiDI,IAAMC,WCaxCwB,E,kDAjBX,aAAc,IAAD,8BACb,gBACKvC,MAAQ,CACZwC,QAAS,IAHG,E,qDAQhB,OACC,yBAAKjC,UAAU,OACf,yBAAKA,UAAU,QACf,kBAAC,EAAD,Y,GAZgBO,IAAMC,WCQlB0B,EAAQ,CACZhB,SAAU,CAER,CACIG,KAAM,iBACNC,IAAK,sDACZG,OAAQ,8FACJZ,UAAW,+FAEZ,CACIQ,KAAM,QACNC,IAAK,wCACLG,OAAQ,2FACRZ,UAAW,4FAEf,CACIQ,KAAM,UACNC,IAAK,mDACZa,WAAY,yCACZV,OAAQ,4GACRZ,UAAW,6GAER,CACIQ,KAAM,qBACNC,IAAK,oDACLG,OAAQ,qFACfZ,UAAW,sFAER,CACIQ,KAAM,aACNC,IAAK,oEACLG,OAAQ,2FACfZ,UAAW,4FAEP,CACGQ,KAAM,WACNC,IAAK,0CACLG,OAAQ,sFACfZ,UAAW,uFAER,CACIQ,KAAM,MACNC,IAAK,2CACLG,OAAQ,iGACXZ,UAAW,kGAEZ,CACIQ,KAAM,kBACNC,IAAK,gDACLG,OAAQ,qFACfZ,UAAW,wFAKNuB,EAAQ,CACZlB,SAAU,CACR,CACIG,KAAM,MACNC,IAAK,qCACLG,OAAQ,6FACXZ,UAAW,8FAEX,CACGQ,KAAM,YACNC,IAAK,2CACLG,OAAQ,yFACfZ,UAAW,0FAER,CACIQ,KAAM,sBACNC,IAAK,mDACLG,OAAQ,+FACfZ,UAAW,gGAER,CACIQ,KAAM,iBACNC,IAAK,+CACLG,OAAQ,yFACfZ,UAAW,0FAER,CACIQ,KAAM,uBACNC,IAAK,uCACLG,OAAQ,sFACfZ,UAAW,uFAER,CACIQ,KAAM,mBACNC,IAAK,kDACLG,OAAQ,gGACRZ,UAAW,mGAMbwB,E,kDAMN,WAAY7C,GAAQ,uCACbA,G,gEAJNI,SAAS0C,MAAQ,mB,8CAQd,OACC,kBAAC,IAAD,KACA,kBAAC,IAAD,KACE,6BAEF,kBAAC,IAAD,CAAOC,OAAK,EAACC,KAAK,IAAIC,OAAQ,kBAEjC,6BACC,yBAAKzC,UAAY,UACf,yBAAKA,UAAY,cAAjB,iBAEC,yBAAKA,UAAY,YAEpB,uBAAGA,UAAY,eAAeG,KAAK,2HAAnC,UAIE,kBAAC,IAAD,CAAMH,UAAY,mBAAmBC,GAAI,cAAzC,mBAMH,yBAAKD,UAAY,aAAjB,IAA8B,kBAAC,EAAD,CAAYiB,WAAciB,IAAxD,UAKI,kBAAC,IAAD,CAAOK,OAAK,EAACC,KAAK,aAAaC,OAAQ,kBAC7B,6BACb,yBAAKzC,UAAY,UACI,yBAAKA,UAAY,cAAjB,iBAEA,yBAAKA,UAAY,YAEjC,uBAAGA,UAAY,eAAeG,KAAK,2HAAnC,UAIA,kBAAC,IAAD,CAAMH,UAAY,mBAAmBC,GAAI,KAAzC,SAKgB,yBAAKD,UAAY,eAAjB,MAGtB,yBAAKA,UAAY,aAAjB,IAA8B,kBAAC,EAAD,CAAYiB,WAAcmB,IAAxD,UAKI,kBAAC,IAAD,CAAOG,OAAK,EAACC,KAAK,WAAWC,OAAQ,kBACnB,yBAAKzC,UAAY,uBAAjB,IAAwC,kBAAC,EAAD,MAAxC,QAGlB,kBAAC,IAAD,CAAOuC,OAAK,EAACC,KAAK,wBAAwBC,OAAQ,kBAChC,yBAAKzC,UAAY,oCAAjB,IAAqD,kBAAC,EAAD,MAArD,e,GApELO,IAAMC,WA+EV6B,cAEfK,IAASD,OAAQ,kBAAC,EAAD,MAAS7C,SAASkB,eAAe,W","file":"static/js/main.e4965ff7.chunk.js","sourcesContent":["import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport PropTypes from \"prop-types\";\nimport ReactPlayer from \"react-player\";\nimport {render} from \"react-dom\";\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport {HashRouter, Link, Switch} from \"react-router-dom\";\nimport '../portfolio.css'; \nimport './../portfolio.css';\nimport './../index.css';\n\nexport default class ROS_Navigation_Stack_Description extends React.Component {\n       constructor(props) {\n       \t\tsuper(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n        }\n\t\n\t// Set the browser tab name\n\tcomponentDidMount(){\t   \t\n\t\tdocument.body.style.backgroundImage = null;\t\n\t}\n\t\n\tstate = {}\n   render () {                                   \n      return ( \n\t\t      <div className = \"Deep_RL_Description\">\n\t      \t     \n\t      \t      \t\t<div className = \"Banner\">\n                \t        \t<div className = \"bannerName\"> Peter Jochem\n                        \t</div>\n                  \n\t      \t\t<div className = \"moreInfo\">\n                        \t<Link className = \"myResumeLink\" to =\"/ResumePage\" >\n                                \tResume\n                        \t</Link>\n\n                        \t<Link className = \"moreProjectsLink\" to =\"/projects2\" >\n                                \tMore Projects\n                        \t</Link>\n                        </div> \n\t      \t      </div>\n\n\t      {/*\n\t      <div className=\"HeaderVideoContainer\">\n\t      \t<ReactPlayer url=\"https://www.youtube.com/watch?v=-Q_36Qi5CVc\" className=\"HeaderVideo\" />\n\t      </div>\n\t      */}\n\n\t      <div id=\"paragraph_div\">\n      \t   \n\t     <p> Insert a grid of images/gifs</p>\n\n\t      <h1> Learning to Walk on Soft Ground </h1>\n\t      <p> This project was motivated by the work of Dan Lynch. He studies optimal control algorithms for legged robots on yielding terrain. Most of the work on legged robotics assumes the ground is a rigid body, but nature is full of materials that exhibit more complicated dynamics. Dans algorithms require a model of how a robots feet interact with the ground. I worked with Juntao He to develop discrete element method (DEM) simulations of a robots feet intruding into a bed of granular material. We then trained a neural network to map the state of the foot to the ground reaction forces and torques exerted by the granular material. This allows us to have a model of the ground which can then be used by Dans optimal control algorithms. </p>\n\n\t      <p>\n\t      \tThe second part of the project involved learning about reinforcement learning, succesively implementing more complicated deep reinforcement learning algorithms, and eventually trying to solve Dans problem via RL. I started out reading Sutton and Barto (link) and building intution about reinforcement learning problems. I implemented DeepQ Learning, and Double DeepQ Learning in order to build up to more complicated temporal diffrence learning algorithms. I then implemented Deep Determinisitc Policy Gradients and then TD3. Finally, I tried to get a working implementation of a Policies Modulating Trajectories Generators (link) architecture for our hopping robot problem.   \n\t      </p>\n\n\t      <h1> Learning a Model of the Ground </h1>\n              <p>\n              The <a className=\"TDynamics_Link\" href={'https://li.me.jhu.edu/'}> Terradynamics lab </a> at Georgia Tech has done a lot of work on how animal and robotic feet interact with granular materials. They studies the terradynamics of robotic locomotion. The lab has done experimental work to show how certain granular materials ground reaction forces change as a function of how the foot is oriented and the direction it is moving through the material. Juntao He and I re-created this experimental setup in a discrete element method (DEM) simulation. We could then generate datasets describing how the ground reaction forces and torques change as the foot intrudes through the material. A visualization of what the experimental setup is below.  \n\t      </p>\n\t\t\n\t      <div class=\"row\">\n                <div class=\"column_for_intrusion_gif\">\n                        <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/media/intrusion.gif\" alt=\"Foot Intruding into the Granular Material\" alt=\"Foot Intruding Into The Granular Material\"/>\n        </div>\n</div>\n\n\n              <p> We used the DEM simulation of the Terradynamics lab experiments to generate a dataset of foots states and the corresponding ground reaction forces and torques as the foot intrudes into the granular material. We want to predict the ground reaction forces and torques as a function of the foots state. Specefically, we want to map the foots angle of attack, orientation angle, depth, x-velocity, z-velocity, and angular velocity to the ground reaction forces and torques that the foot experiences. Other approaches to this include <a className=\"RFT_Link\" href={'https://li.me.jhu.edu/first-terradynamics-resistive-force-theory/'}> resistive force theory (RFT) </a> but even this very sophisticated method does not utilize the foots velocity information. Neural networks offer a computationally tractable way to learn this function taking into account information that RFT is not suited to use.\n              </p>\n\t<p>\n\t\tWe used our DEM dataset to train a neural network to map the foots state to the corresponding ground reaction forces and torques that the ground exerts on the foot. Below are our comparisons of the neural networks predictions to the ground truth DEM data and the RFT calculations. The curves below are from a single run of the foot interacting with the granular material. At each time step, we recorded the foots state along with ground reaction forces and torques. The yellow curve below are the forces and torques as the DEM simulation calcuated them. In blue are the RFT predictions of what the forces and torques should be given as input the current DEM simulations foot state. We plotted in orange the neural network predictions of what the forces and torques should be given the current DEM simulations foot state.  \n\t      </p>\n\n\t\t\n\t<div class=\"row\">\n        \t<div class=\"column\">\n                \t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/validateModel/DEM_RFT_Comparisons/media/velocity_minus_2_results.png\" alt=\"Learned Mapping Compared to DEM and RFT\"/>\n        </div>\n</div>\n\n\t\n\t     \t\t\t\n\n\n\t\t\n\t      <h1> Learning to Walk On Soft Ground </h1>\n\t      <p> In order to test Deep RL algorithms for the robot hopping on soft ground, I built a custom OpenAI gym environment. The OpenAI gym project was started in order to facilitate a common standard or benchmark for comparing RL algorithms. An agent has a set of actions it can take to influence the environments state and accumulate reward. The agent must learn how to take actions which maximize its cumulative reward over time. One of the more simple gym environments is the pendulum environment. It features a simple pendulum that the agent must learn how to invert. At each time step, the agent recieves the pendulums angle and can choose to either exert a unit of positive torque or negative torque. Although the problem can be easily solved with a PID controller, it serves as a simple test to see if your agent is learning. I built my gym in <a className=\"paper_link\" href=\"https://pybullet.org/wordpress/\"> PyBullet </a>, an open source physics engine. It features a hopping robot with an open chain leg. In high level terms, the agents goal is move in the postive x-direction. I found that the exact details of the agents reward function made a huge diffrence in the agents outcome. Roughly speaking, I rewarded the agent for having a larger x-coordinate, a positive x-velocity, and not falling over. I applied both <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> and <a className=\"paper_link\" href=\"https://arxiv.org/pdf/1802.09477.pdf\"> TD3 </a> to the hopping robot and got interesting results! The agent decided to lock its leg joints and use its foot to generate ground reaction forces. Although this is technically a viable policy, it is far from the natural gait we might have hoped for.                        \n\t      </p>\n\n\t      <p>\n\t\t<a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> (DDPG) is one of the state of the art RL algorithms for continous action spaces. Most of the work in RL has focused on agents who have a discrete number of actions they can take. Some of these discrete action space methods include <a className=\"paper_link\" href=\"https://en.wikipedia.org/wiki/Q-learning\"> Q-Learning </a>, <a className=\"paper_link\" href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\"> DeepQ Networks </a>, and <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.06461\"> Double DeepQ Learning </a>. In our case though, we want to learn how to apply torques to each of the robots motors. We could discretize the set of allowable motor torques but in practice, this does not work well. Instead, we need to use an algorithm designed for continous action spaces. I implemented both DDPG and TD3 for the hopping robot. In order to facilitate testing of DDPG and TD3, I also tried them on the Cheetah and Hopper Environments published by the OpenAI team. Both feature a legged robot that must learn how to locomote by applying a set of motor torques on its joints. This helped me validate the RL algorithms and was also a lot of fun. Below are some of the gaits that the agents learned.         \n\t      </p>\n\n<div class=\"row\">\n \t<div class=\"column\">\n   \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy.gif\" alt=\"Learned Gaits in Gym Environments\"/>\n\t</div>\n\t<div class=\"column\">\n  \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/cheetah2.gif\" alt=\"Learned Gait in OpenAI Gym\" className=\"OpenAI_gym_results_gif2\"/>\n \t</div>\n</div>\n\n\t      <img src=\" https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/h3pper_DDPG2.gif\" alt=\"AI Gym Hopping Robot\" className=\"intrusion_gif\" />\n\t\t\n\t\t</div>\n\t   </div>\t      \n      )\n   }\n}\n\n// Type Check properties here\n//Deep_RL_Description.propTypes = {\n//};\n//Deep_RL_Description.defaultProps = {\n//};\n\n\n","import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport PropTypes from \"prop-types\";\nimport {render} from \"react-dom\";\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport './portfolio.css'; \n\nclass GridOfPosts extends React.Component {        \n\tconstructor(props) {\n                super(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n                this.onMouseEnterHandler = this.onMouseEnterHandler.bind(this);\n        \tthis.onMouseExitHandler = this.onMouseExitHandler.bind(this);\n\t}\n\n\tcomponentDidMount(){}\n\t\n\tonMouseEnterHandler(projectName, hover_url) {\n                var myElement = document.getElementById(projectName);\n                myElement.style.backgroundImage = \"url(\" + hover_url + \")\";\n        }\n\n\tonMouseExitHandler(projectName, static_url) {\n\t\tvar myElement = document.getElementById(projectName);\n                myElement.style.backgroundImage = \"url(\" + static_url + \")\";\t\n\t}\n\n\n   render () {\n      return (\n\t\n\t      <div className = \"GridOfPosts\"> \n              <div className=\"grid-container\">\n\t   \t\n\t      {this.props.pageNumber.projects.map( (project) => {\n                        return  <div className = \"box\">\n\n\t\t\t      \t{project.name}\n\t\t\t\t<a href = {project.url} onMouseEnter={() => this.onMouseEnterHandler(project.name, project.hover_url)} \n\t\t      \t\t\t\t\tonMouseLeave={() => this.onMouseExitHandler(project.name, project.im_url)} > \n              \t\t \t\t<div className=\"grid-item\" id = {project.name}>  <Post name={project.name} im_url = {project.im_url} hover_url = {project.hover_url}\n\t\t\t      \t\t/> </div> \n\t\t\t\t</a>             \n\t\t\t </div>\n                }\n                )}\n\t      </div>   \t \n\t      </div>\n      )\n   }\n}\n\nGridOfPosts.propTypes = {\n        pageNumber: PropTypes.node.isRequired,\n};\n\nGridOfPosts.defaultProps = {\n\tpageNumber: null,\n};\n\n\nclass Post extends React.Component {\n\tconstructor(props) {\n   \t\tsuper(props);\t\n\t\tthis.componentDidMount = this.componentDidMount.bind(this);\n\t}\n\t\n\tstate = {i: 0}\n\n        componentDidMount(){\n\t\tvar myElement = document.getElementById(this.props.name);\t\n\t\tmyElement.style.backgroundImage = \"url(\" + this.props.im_url + \")\";\n\t}\n\t\n\n   render () {\n      return (\n        <div className = \"Post\"/>\n      )\n   }\n}\n\n// Type check the props\nPost.propTypes = {\n\t// No built in image type, image: React.PropTypes.image\n\t// Alternative would be to store the URL/file location of the image\n\tname: PropTypes.string.isRequired,\n\tim_url: PropTypes.string.isRequired,\n\thover_url: PropTypes.string.isRequired\n};\n\nPost.defaultProps = {\n\tname: 'me',\n\tim_url: ''\n};\n\nexport default class Portfolio extends React.Component {\n       constructor(props) {\n       \t\tsuper(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n        }\n\t\n\tcomponentDidMount(){\n\t\tdocument.body.style.backgroundImage = null;\t\n\t}\n\t\n\tstate = {}\n   render () {                                   \n      return (\n\t \n\t      <div>\n\t      \t<div className = \"portfolio\"> \n\t      \t\t<GridOfPosts pageNumber={this.props.pageNumber} />\t\n\t      </div>\n\t      </div>\n      )\n   }\n}\n\n// Type check the props\nPortfolio.propTypes = {\n\tpageNumber: PropTypes.element.isRequired,\n};\n\nPortfolio.defaultProps = {\n\t/* This describes if we are using the first or second page\n\tpageNumber: 1 */\n};\n","import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport PropTypes from \"prop-types\";\nimport ReactPlayer from \"react-player\";\nimport {render} from \"react-dom\";\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport {HashRouter, Link, Switch} from \"react-router-dom\";\nimport '../portfolio.css'; \nimport './../portfolio.css';\nimport './../index.css';\n\nexport default class Deep_RL_Description extends React.Component {\n       constructor(props) {\n       \t\tsuper(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n        }\n\t\n\tcomponentDidMount(){\t   \t\n\t\tdocument.body.style.backgroundImage = null;\t\n\t}\n\t\n\tstate = { \n\t}\n   render () {                                   \n      return ( \n\t\t      <div className = \"Deep_RL_Description\">\n\t      \t     \n\t      \t      \t\t<div className = \"Banner\">\n                \t        \t<div className = \"bannerName\"> Peter Jochem\n                        \t</div>\n                  \n\t      \t\t<div className = \"moreInfo\">\n\t\t\t\t<a className = \"myResumeLink\" href=\"https://raw.githubusercontent.com/PeterJochem/PeterJochem.github.io/6331c077619a0b5593287d0fcead9a932102b569/Resume.pdf\" >\n                               \t\tResume\n                        \t</a>\n\t\t\t\n                        \t<Link className = \"moreProjectsLink\" to =\"/projects2\" >\n                                \tMore Projects\n                        \t</Link>\n                        </div> \n\t      \t      </div>\n\n\t     <div className=\"HeaderVideoContainer\" > \n\t\t     <ReactPlayer url=\"https://www.youtube.com/watch?v=llKHSVy_aUU&feature=youtu.be\" controls=\"True\" className=\"HeaderVideo\" />\n\t     </div>\n\n\t     <div id=\"paragraph_div\">\n      \t   \n\t      <h1> Learning to Walk on Soft Ground - Overview </h1>\n\t      <p> This project was motivated by the work of <a href=\"https://robotics.northwestern.edu/people/profiles/students/lynch-dan.html\" className=\"paper_link\"> Dan Lynch</a>. He studies optimal control algorithms for legged robots on yielding terrain. Most of the work on legged robotics assumes the ground is a rigid body, but nature is full of materials that exhibit more complicated dynamics. Dan&apos;s algorithms require a model of how a robot&apos;s feet interact with the ground. I worked with Juntao He to develop discrete element method (DEM) simulations of a robot&apos;s feet intruding into a bed of granular material. We then trained a neural network to map the state of the foot to the ground reaction forces and torques exerted by the granular material. This allows us to have a model of the ground which can then be used by Dan&apos;s optimal control algorithms. For more technical details, feel free to check out the <a href=\"https://github.com/PeterJochem/Deep_RL\" className=\"paper_link\"> Github repo</a> </p>\n\n\t      <p>\n\n\t      \tThe second part of the project involved learning about reinforcement learning, succesively implementing more complicated deep reinforcement learning algorithms, and eventually trying to solve Dan&apos;s problem via RL. I started out reading <a className=\"paper_link\" href=\"http://incompleteideas.net/book/the-book.html\"> Sutton&apos;s and Barto&apos;s Reinforcement Learning </a> and building intution about reinforcement learning problems. I implemented <a className=\"paper_link\" href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\"> DeepQ Learning </a>, and <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.06461\"> Double DeepQ Learning </a> in order to build up to more complicated temporal diffrence learning algorithms. I then implemented <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> and <a className=\"paper_link\" href=\"https://arxiv.org/pdf/1802.09477.pdf\"> TD3 </a>. Finally, I tried to get a working implementation of a <a className=\"paper_link\" href=\"https://arxiv.org/abs/1910.02812\"> Policies Modulating Trajectories Generators </a> architecture for our hopping robot problem.   \n\t      </p>\n\n\t      <h1> Learning a Model of the Ground </h1>\n              <p>\n              The <a className=\"TDynamics_Link\" href={'https://li.me.jhu.edu/'}> Terradynamics lab </a> at Georgia Tech has done a lot of work on how animal and robotic feet interact with granular materials. They study the terradynamics of robotic locomotion. The lab has done experimental work to show how certain granular materials ground reaction forces change as a function of how the foot is oriented and the direction it is moving through the material. Juntao He and I re-created this experimental setup in a discrete element method (DEM) simulation. We could then generate datasets describing how the ground reaction forces and torques change as the foot intrudes through the material. A visualization of what the experimental setup is below.  \n\t      </p>\n\t\t\n\t      <div class=\"row\">\n                <div class=\"column_for_intrusion_gif\">\n                        <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/media/intrusion.gif\" alt=\"Foot Intruding into the Granular Material\" alt=\"Foot Intruding Into The Granular Material\"/>\n        </div>\n</div>\n\n\n              <p> We used the DEM simulation of the Terradynamics lab experiments to generate a dataset of foot states and the corresponding ground reaction forces and torques as the foot intrudes into the granular material. We want to predict the ground reaction forces and torques as a function of the foot&apos;s state. Specifically, we want to map the foot&apos;s angle of attack, orientation angle, depth, x-velocity, z-velocity, and angular velocity to the ground reaction forces and torques that the foot experiences. Other approaches to this include <a className=\"RFT_Link\" href={'https://li.me.jhu.edu/first-terradynamics-resistive-force-theory/'}> resistive force theory (RFT) </a> but even this very sophisticated method does not utilize the foot&apos;s velocity information. Neural networks offer a computationally tractable way to learn this function taking into account information that RFT is not suited to use.\n              </p>\n\t<p>\n\t\tWe used our DEM dataset to train a neural network to map the foot&apos;s state to the corresponding forces and torques that the ground exerts on the foot. Below are our comparisons of the neural network&apos;s predictions to the ground truth DEM data and the RFT calculations. The curves below are from a single run of the foot interacting with the granular material. At each time step, we recorded the foot&apos;s state along with the ground reaction forces and torques. The yellow curves below are the forces and torques as the DEM simulation calcuated them. In blue are the RFT predictions of the forces and torques, given as input the current DEM simulation&apos;s foot state. In orange is the neural network&apos;s predictions of what the forces and torques should be given the current DEM simulation&apos;s foot state.  \n\t      </p>\n\n\t\t\n\t<div class=\"row\">\n        \t<div class=\"column\">\n                \t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/validateModel/DEM_RFT_Comparisons/media/velocity_minus_2_results.png\" alt=\"Learned Mapping Compared to DEM and RFT\"/>\n        </div>\n</div>\n\n<h1> Learning to Walk On Soft Ground </h1>\n\n\t      <p> In order to test Deep RL algorithms for the robot hopping on soft ground, I built a custom OpenAI gym environment. The OpenAI gym project was started in order to facilitate a common standard for comparing RL algorithms. An agent has a set of actions it can take to influence the environment&apos;s state and accumulate reward. The agent must learn how to take actions which maximize its cumulative reward over time. One of the simpler gym environments is the pendulum environment. It features a simple pendulum that the agent must learn how to invert. At each time step, the agent recieves the pendulum&apos;s angle and can choose to either exert a unit of positive torque or negative torque. Although the problem can be easily solved with a PID controller, it serves as a simple test to see if your agent is learning. I built my gym in <a className=\"paper_link\" href=\"https://pybullet.org/wordpress/\"> PyBullet </a>, an open source physics engine. It features a hopping robot with an open chain leg. What makes my environment different is that I use my model of the soft ground to govern the ground reaction forces and torques on the robot&apos;s foot. In PyBullet, I track where the foot is, and when it is in contact with the granular material, I apply the ground reaction forces and torques that the model predicts. This allows me to have a highly tractable and realistic simulation of a monoped hopping on soft ground. </p>\n\t\n\t\n\t<p>\n\t      In high level terms, the agent&apos;s goal is to move in the postive x-direction. I found that the exact details of the agent&apos;s reward function made a huge difference in the agent&apos;s outcome. Roughly speaking, I rewarded the agent for having a larger x-coordinate, a positive x-velocity, and not falling over. I applied both <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> and <a className=\"paper_link\" href=\"https://arxiv.org/pdf/1802.09477.pdf\"> TD3 </a> to the hopping robot and got interesting results! The agent decided to lock its leg joints and use only its foot to generate ground reaction forces. Although this is technically a viable policy, it is far from the natural gait we might have hoped for.                        \n\t</p>\n\n\t      <p>\n\t\t<a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> (DDPG) is one of the state of the art RL algorithms for continous action spaces. Most of the work in RL has focused on agents who have a discrete number of actions they can take. Some of these discrete action space methods include <a className=\"paper_link\" href=\"https://en.wikipedia.org/wiki/Q-learning\"> Q-Learning </a>, <a className=\"paper_link\" href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\"> DeepQ Networks </a>, and <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.06461\"> Double DeepQ Learning </a>. In our case though, we want to learn how to apply torques to each of the robot&apos;s motors. We could discretize the set of allowable motor torques but in practice, this does not work well. Instead, we need to use an algorithm designed for continous action spaces. I implemented both DDPG and TD3 for the hopping robot. In order to facilitate testing of DDPG and TD3, I also tried them on the Cheetah and Hopper Environments published by the OpenAI team. Both feature a legged robot that must learn how to locomote by applying a set of motor torques on its joints. This helped me validate the RL algorithms and was also a lot of fun. Below are some of the gaits that the agents learned.         \n\t      </p>\n\n<div class=\"row\">\n \t<div class=\"column\">\n   \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy.gif\" alt=\"Learned Gaits in Gym Environments\"/>\n\t</div>\n\t<div class=\"column\">\n  \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/cheetah2.gif\" alt=\"Learned Gait in OpenAI Gym\" className=\"OpenAI_gym_results_gif2\"/>\n \t</div>\n</div>\n\n<p className=\"p_describe_pyBullet_ddpg\">\n\tOnce I had validated my implementation of DDPG using the OpenAI team&apos;s environments, I applied the algorithm to my custom environment in PyBullet. My hopping robot environment approximates Dan&apos;s robot. As of 2020, PyBullet does not support granular materials. So, I used the ground model from part one to simulate the soft ground in PyBullet. This allowed me to have a computationally tractable model of soft ground which I could use for reinforcement learning. I used both DDPG and TD3 in the new environment. Below, on the left, are some of the learned gaits. Below on the right is a visualization of the soft ground I later added.             \n</p>\n\n\n<div class=\"row\">\n        <div class=\"column\">\n                <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/h3pper_DDPG2.gif\" alt=\"Learned Gaits in Gym Environments\"/>\n        </div>\n        <div class=\"column\">\n                <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/gym-hopping_robot/images/hopper_in_sand.png\" alt=\"Learned Gait in OpenAI Gym\" className=\"OpenAI_gym_results_gif2\"/>\n        </div>\n</div>\n\n<p className=\"p_describe_pyBullet_pmtg\">\n\tI then tried to implement a <a className=\"paper_link\" href=\"https://arxiv.org/abs/1910.02812\"> Policies Modulating Trajectory Generators </a> architecture for the hopping robot on soft ground. This architecture allows us to incorporate prior insight about what the robot&apos;s desired trajectory should be. The agent learns to control a new dynamical system, that of the robot and its controller rather than the robot directly. This results in learning good policies much faster. We modified Dan&apos;s optimal open-loop controller to be our trajectory generator. At each time step, the agent can modify parameters that define the open loop controller as well as add modulation terms to the ouput of the trajectory generator. In our case, our agent has a trajectory generator which is very well suited for the problem at hand but the trajectory generator need not be so well engineered. The original PMTG paper used a relatively approximate trajectory generator and still achieved good results. Think of the the trajectory generator as being near the desired solution and then letting the agent learn how to modify it to work fully. Unfortunately, I got the software setup but could not get the entire system to work!                             \n</p>\n\n</div>\n</div>\n      )\n   }\n}\n\n","import React from \"react\";\nimport { render } from \"react-dom\";\nimport ReactDOM from 'react-dom';\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport Site from \"./index.js\";\n\nclass App extends React.Component {\n    constructor(){\n    super();\n    this.state = {\n     results: {}\n    }\n    }\n \n   render () {\n\treturn (\n\t\t<div className=\"App\">\n\t\t<div className=\"Site\">\n\t\t<Site />\n\t\t</div>\n\t\t</div>\n\t)\n   }\n}\nexport default App;\n// ReactDOM.render(<BrowserRouter basename={process.env.PUBLIC_URL}>< App /></BrowserRouter>, document.getElementById('root'));\n","import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport {render} from \"react-dom\";\nimport {HashRouter, Route, Link, Switch} from \"react-router-dom\";\nimport './index.css'; \nimport './DescriptionPages/Deep_RL_Description.css'\nimport './DescriptionPages/ROS_Navigation_Stack_Description'\n// import './DescriptionPages/Deep_RL_Description.css'\nimport './portfolio.css';\nimport Portfolio from \"./portfolio\";\nimport Deep_RL_Description from \"./DescriptionPages/Deep_RL_Description\"\nimport ROS_Navigation_Stack_Description from \"./DescriptionPages/ROS_Navigation_Stack_Description\"\nimport App from \"./App.js\";\n\nconst page1 = {\n  projects: [\n    \n    {\n        name: \"ROS Navigation\",\n        url: \"https://github.com/PeterJochem/Turtlebot_Navigation\",\n\tim_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/tbot_pentagon.gif\",\n    \thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/tbot_pentagon.gif\"\n    },\n    {\n        name: \"GoBot\",\n        url: \"https://github.com/PeterJochem/Go_Bot\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/GoBot_Demo.gif\",\n        hover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/GoBot_Demo.gif\"\n     },\n    {\n        name: \"Deep RL\",\n        url: \"https://peterjochem.github.io/Portfolio#/Deep_RL\",\n\tgithub_url: \"https://github.com/PeterJochem/Deep_RL\",\n\tim_url: \"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy_cropped.gif\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy_cropped.gif\"\n    },\n    {\n        name: \"Mobile Manipulator\",\n        url: \"https://github.com/PeterJochem/Mobile_Manipulator\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/kuka.gif\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/kuka.gif\"\n    },\n    {\n        name: \"Terminator\",\n        url: \"https://github.com/ME495-EmbeddedSystems/final-project-terminator\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/terminator.png\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/terminator.png\"\n     },\n     {\n        name: \"Chess AI\",\n        url: \"https://github.com/PeterJochem/Chess_AI\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/chess.gif\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/chess.gif\"\n    }, \n    {\n        name: \"GAN\",\n        url: \"https://github.com/PeterJochem/MNIST_GAN\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/GAN_downsampled3.gif\",\n    \thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/GAN_downsampled3.gif\"\n    },\n    {\n       \tname: \"Triple Pendulum\",\n        url: \"https://github.com/PeterJochem/TriplePendulum\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/pend.gif\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/pend.gif\"\n    },\n]\n}\n\nconst page2 = {\n  projects: [\n    {\n        name: \"RRT\",\n        url: \"https://github.com/PeterJochem/RRT\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/all_Points_3.png\",\n    \thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/all_Points_3.png\"\n    },\n     {\n        name: \"CBirch 97\",\n        url: \"https://github.com/PeterJochem/CBirch_97\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/CBirch97.gif\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/CBirch97.gif\"\n    },\n    {\n        name: \"Canny Edge Detector\",\n        url: \"https://github.com/PeterJochem/CannyEdgeDetector\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/Lena_Processed.png\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/Lena_Processed.png\"\n    }, \n    {\n        name: \"DeepQ Learning\",\n        url: \"https://github.com/PeterJochem/Grid_World_RL\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/NN_Large.png\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/NN_Large.png\"\n    },\n    {\n        name: \"Neural Network Snake\",\n        url: \"https://github.com/PeterJochem/Snake\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/Snake.png\",\n\thover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/Snake.png\"\n    },\n    {\n        name: \"Sawyer Ping Pong\",\n        url: \"https://github.com/PeterJochem/Sawyer_Ping_Pong\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/ping_trajectory.png\",\n        hover_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/ping_trajectory.png\"\n     }\n  ]\n}\n\n\nclass Site extends React.Component {\n\ncomponentDidMount(){\n\tdocument.title = \"Peter Jochem\" // Set the browser tab name\n}\n\nconstructor(props) {\n\tsuper(props);\n}\n\n  render() {\n    return (\n\t    <HashRouter>\n\t    <Switch>\n       <div>\n          \n\t    <Route exact path='/' render={() => (\n            \t\n\t\t<div>\n\t\t\t<div className = \"Banner\"> \t\n\t\t  \t<div className = \"bannerName\"> Peter Jochem \n\t\t  \t</div>\n\t\t   \t<div className = \"moreInfo\">\n\t\t  \t\n\t\t\t<a className = \"myResumeLink\" href=\"https://raw.githubusercontent.com/PeterJochem/PeterJochem.github.io/6331c077619a0b5593287d0fcead9a932102b569/Resume.pdf\" >\n                                Resume\n                        </a>\n\n\t\t  \t<Link className = \"moreProjectsLink\" to =\"/projects2\" >  \n\t\t    \t\tMore Projects\n\t\t    \t</Link>\n\t\t        </div>\t\n\t\t</div>\n\n\t\t<div className = \"portfolio\"> <Portfolio  pageNumber = {page1} />  </div>\t\n\t\t</div>\t\n\n    \t\t)} />\n\t\t\n\t     <Route exact path='/projects2' render={() => (\n                <div>\n\t\t\t<div className = \"Banner\">\n                        <div className = \"bannerName\"> Peter Jochem\n                        </div>\n                        <div className = \"moreInfo\">\n\n\t\t     \t<a className = \"myResumeLink\" href=\"https://raw.githubusercontent.com/PeterJochem/PeterJochem.github.io/6331c077619a0b5593287d0fcead9a932102b569/Resume.pdf\" >\n                                Resume\n                        </a>\n\n\t\t     \t<Link className = \"moreProjectsLink\" to =\"/\" >\n                                Main\n                        </Link>\n\t\t     \t\n                        </div>\n                        <div className = \"BannerImage\"> </div>\n                </div>\n                \n\t\t<div className = \"portfolio\"> <Portfolio  pageNumber = {page2} />  </div>\t\t\n\t\t</div>\n\n                )} />\n\n\t    \t<Route exact path='/Deep_RL' render={() => (\n                        <div className = \"Deep_RL_Description\"> <Deep_RL_Description /> </div>\n                )} />\n\n\t    \t<Route exact path='/ROS_Navigation_Stack' render={() => (\n                        <div className = \"ROS_Navigation_Stack_Description\"> <ROS_Navigation_Stack_Description /> </div>\n                )} />\n\n\t  \n\t    \n\t  </div>\n\t  </Switch>\n\t  </HashRouter>\n    );\n  }\n}\nexport default Site;\n\nReactDOM.render( <App />, document.getElementById('root') );\n"],"sourceRoot":""}