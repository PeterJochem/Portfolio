{"version":3,"sources":["DescriptionPages/ROS_Navigation_Stack_Description.js","portfolio.js","DescriptionPages/Deep_RL_Description.js","App.js","index.js"],"names":["ROS_Navigation_Stack_Description","props","state","componentDidMount","bind","document","body","style","backgroundImage","className","to","id","href","class","src","alt","React","Component","GridOfPosts","this","pageNumber","projects","map","project","name","url","im_url","defaultProps","Post","getElementById","Portfolio","Deep_RL_Description","controls","App","results","page1","github_url","page2","Site","title","exact","path","render","width","height","frameborder","ReactDOM"],"mappings":"iVAWqBA,G,wDACd,WAAYC,GAAQ,IAAD,8BACjB,cAAMA,IASdC,MAAQ,GARO,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFf,E,gEAOxBC,SAASC,KAAKC,MAAMC,gBAAkB,O,+BAKlC,OACE,yBAAKC,UAAY,uBAET,yBAAKA,UAAY,UACP,yBAAKA,UAAY,cAAjB,iBAGjB,yBAAKA,UAAY,YACD,kBAAC,IAAD,CAAMA,UAAY,eAAeC,GAAI,eAArC,UAIA,kBAAC,IAAD,CAAMD,UAAY,mBAAmBC,GAAI,cAAzC,mBAYlB,yBAAKC,GAAG,iBAET,4DAEC,iEACA,4vBAEA,isBAIA,gEACO,kCACI,uBAAGF,UAAU,iBAAiBG,KAAM,0BAApC,uBADJ,4oBAIP,yBAAKC,MAAM,OACF,yBAAKA,MAAM,4BACH,qCAAKC,IAAI,iHAAiHC,IAAI,6CAA9H,MAA8K,gDAKxL,gjBAAqhB,uBAAGN,UAAU,WAAWG,KAAM,qEAA9B,kCAArhB,yOAEb,m1BAKA,yBAAKC,MAAM,OACH,yBAAKA,MAAM,UACH,yBAAKC,IAAI,kKAAkKC,IAAI,8CASzL,iEACA,y2BAA80B,uBAAGN,UAAU,aAAaG,KAAK,mCAA/B,cAA90B,gZAA0yC,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCAA1yC,QAA05C,uBAAGH,UAAU,aAAaG,KAAK,wCAA/B,SAA15C,2PAGA,2BACL,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCADK,2OAC8U,uBAAGH,UAAU,aAAaG,KAAK,4CAA/B,gBAD9U,KAC0a,uBAAGH,UAAU,aAAaG,KAAK,kDAA/B,oBAD1a,SACohB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,2BADphB,+qBAIP,yBAAKC,MAAM,OACT,yBAAKA,MAAM,UACR,yBAAKC,IAAI,oGAAoGC,IAAI,uCAErH,yBAAKF,MAAM,UACR,yBAAKC,IAAI,uFAAuFC,IAAI,6BAA6BN,UAAU,8BAIxI,yBAAKK,IAAI,4FAA4FC,IAAI,uBAAuBN,UAAU,wB,GA/FnFO,IAAMC,YCH9DC,E,4MASEhB,MAAQ,G,oGAIV,OAEC,yBAAKO,UAAY,eAEV,yBAAKA,UAAU,kBAErBU,KAAKlB,MAAMmB,WAAWC,SAASC,KAAK,SAACC,GACrB,OAAQ,yBAAKd,UAAY,OAAjB,KAA0Bc,EAAQC,KAC/D,uBAAGZ,KAAQW,EAAQE,KACP,yBAAKhB,UAAU,YAAYE,GAAMY,EAAQC,MAAzC,KAAkD,kBAAC,EAAD,CAAMA,KAAMD,EAAQC,KAAOE,OAAUH,EAAQG,SAA/F,OADZ,e,GArBuBV,IAAMC,WAsChCC,EAAYS,aAAe,CAC1BP,WAAY,M,IAIPQ,E,kDACL,WAAY3B,GAAQ,IAAD,8BACd,cAAMA,IAaJC,MAAQ,GAXd,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAHP,E,gEASFC,SAASwB,eAAeV,KAAKlB,MAAMuB,MAEzCjB,MAAMC,gBAAkB,OAASW,KAAKlB,MAAMyB,OAAS,M,+BAU3D,OACE,yBAAKjB,UAAY,a,GAvBNO,IAAMC,WAsCzBW,EAAKD,aAAe,CACnBH,KAAM,KACNE,OAAQ,I,IAIYI,E,kDACd,WAAY7B,GAAQ,IAAD,8BACjB,cAAMA,IAYdC,MAAQ,GAXO,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFf,E,gEAUxBC,SAASC,KAAKC,MAAMC,gBAAkB,O,+BAMlC,OAEC,6BAEA,yBAAKC,UAAY,aACf,kBAAC,EAAD,CAAaW,WAAYD,KAAKlB,MAAMmB,mB,GAtBNJ,IAAMC,WAkC7Ca,EAAUH,aAAe,G,ICtHJI,E,kDACd,WAAY9B,GAAQ,IAAD,8BACjB,cAAMA,IAQdC,MAAQ,GAPO,EAAKC,kBAAoB,EAAKA,kBAAkBC,KAAvB,gBAFf,E,gEAMxBC,SAASC,KAAKC,MAAMC,gBAAkB,O,+BAMlC,OACE,yBAAKC,UAAY,uBAET,yBAAKA,UAAY,UACP,yBAAKA,UAAY,cAAjB,iBAGjB,yBAAKA,UAAY,YACD,kBAAC,IAAD,CAAMA,UAAY,eAAeC,GAAI,eAArC,UAIA,kBAAC,IAAD,CAAMD,UAAY,mBAAmBC,GAAI,cAAzC,mBAOnB,kBAAC,IAAD,CAAae,IAAI,+BAA+BO,SAAS,OAAOvB,UAAU,gBAE1E,yBAAKE,GAAG,iBAEP,iEACA,4vBAEA,4WAE+U,uBAAGF,UAAU,aAAaG,KAAK,kDAA/B,oBAF/U,SAEyb,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,2BAFzb,wGAE2nB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCAF3nB,aAEgvB,uBAAGH,UAAU,aAAaG,KAAK,wCAA/B,SAFhvB,2DAEu3B,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,iDAFv3B,gDAKA,gEACO,kCACI,uBAAGH,UAAU,iBAAiBG,KAAM,0BAApC,uBADJ,4oBAIP,yBAAKC,MAAM,OACF,yBAAKA,MAAM,4BACH,qCAAKC,IAAI,iHAAiHC,IAAI,6CAA9H,MAA8K,gDAKxL,gjBAAqhB,uBAAGN,UAAU,WAAWG,KAAM,qEAA9B,kCAArhB,yOAEb,m1BAKA,yBAAKC,MAAM,OACH,yBAAKA,MAAM,UACH,yBAAKC,IAAI,kKAAkKC,IAAI,8CAKzL,iEACA,y2BAA80B,uBAAGN,UAAU,aAAaG,KAAK,mCAA/B,cAA90B,0hBAIN,sVAC8T,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCAD9T,QAC8a,uBAAGH,UAAU,aAAaG,KAAK,wCAA/B,SAD9a,2PAIM,2BACL,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,yCADK,2OAC8U,uBAAGH,UAAU,aAAaG,KAAK,4CAA/B,gBAD9U,KAC0a,uBAAGH,UAAU,aAAaG,KAAK,kDAA/B,oBAD1a,SACohB,uBAAGH,UAAU,aAAaG,KAAK,oCAA/B,2BADphB,+qBAIP,yBAAKC,MAAM,OACT,yBAAKA,MAAM,UACR,yBAAKC,IAAI,oGAAoGC,IAAI,uCAErH,yBAAKF,MAAM,UACR,yBAAKC,IAAI,uFAAuFC,IAAI,6BAA6BN,UAAU,8BAI/I,uBAAGA,UAAU,4BAAb,yKAKA,yBAAKI,MAAM,OACV,yBAAKA,MAAM,2BACY,yBAAKC,IAAI,2FAA2FC,IAAI,0C,GAnG/EC,IAAMC,WCaxCgB,E,kDAjBX,aAAc,IAAD,8BACb,gBACK/B,MAAQ,CACZgC,QAAS,IAHG,E,qDAQhB,OACC,yBAAKzB,UAAU,OACf,yBAAKA,UAAU,QACf,kBAAC,EAAD,Y,GAZgBO,IAAMC,WCQlBkB,EAAQ,CACZd,SAAU,CAER,CACIG,KAAM,iBACNC,IAAK,sDACLC,OAAQ,sGAEZ,CACIF,KAAM,UACNC,IAAK,mDACZW,WAAY,yCACZV,OAAQ,6GAEL,CACIF,KAAM,qBACNC,IAAK,oDACLC,OAAQ,mDAEZ,CACIF,KAAM,aACNC,IAAK,oEACLC,OAAQ,yDAEX,CACGF,KAAM,WACNC,IAAK,0CACLC,OAAQ,oDAEZ,CACIF,KAAM,mBACNC,IAAK,kDACLC,OAAQ,8DAEX,CACGF,KAAM,iBACNC,IAAK,+CACLC,OAAQ,uDAEZ,CACIF,KAAM,kBACNC,IAAK,gDACLC,OAAQ,qDAKVW,EAAQ,CACZhB,SAAU,CACR,CACIG,KAAM,MACNC,IAAK,qCACLC,OAAQ,2DAEX,CACGF,KAAM,YACNC,IAAK,2CACLC,OAAQ,uDAEZ,CACIF,KAAM,sBACNC,IAAK,mDACLC,OAAQ,6DAEZ,CACIF,KAAM,MACNC,IAAK,2CACLC,OAAQ,qFAEZ,CACIF,KAAM,uBACNC,IAAK,uCACLC,OAAQ,sDAMVY,E,kDAON,WAAYrC,GAAQ,uCACbA,G,gEAJNI,SAASkC,MAAQ,mB,8CAQd,OACC,kBAAC,IAAD,KACA,kBAAC,IAAD,KACE,6BAEF,kBAAC,IAAD,CAAOC,OAAK,EAACC,KAAK,IAAIC,OAAQ,kBAEjC,6BACC,yBAAKjC,UAAY,UACf,yBAAKA,UAAY,cAAjB,iBAEC,yBAAKA,UAAY,YAEpB,kBAAC,IAAD,CAAMA,UAAY,eAAeC,GAAI,eAArC,UAIE,kBAAC,IAAD,CAAMD,UAAY,mBAAmBC,GAAI,cAAzC,mBAMH,yBAAKD,UAAY,aAAjB,IAA8B,kBAAC,EAAD,CAAYW,WAAce,IAAxD,UAKI,kBAAC,IAAD,CAAOK,OAAK,EAACC,KAAK,aAAaC,OAAQ,kBAC7B,6BACb,yBAAKjC,UAAY,UACI,yBAAKA,UAAY,cAAjB,iBAEA,yBAAKA,UAAY,YAEjC,kBAAC,IAAD,CAAMA,UAAY,eAAeC,GAAI,eAArC,UAIA,kBAAC,IAAD,CAAMD,UAAY,mBAAmBC,GAAI,KAAzC,kBAKgB,yBAAKD,UAAY,eAAjB,MAGtB,yBAAKA,UAAY,aAAjB,IAA8B,kBAAC,EAAD,CAAYW,WAAciB,IAAxD,UAKI,kBAAC,IAAD,CAAOG,OAAK,EAACC,KAAK,WAAWC,OAAQ,kBACnB,yBAAKjC,UAAY,uBAAjB,IAAwC,kBAAC,EAAD,MAAxC,QAGlB,kBAAC,IAAD,CAAO+B,OAAK,EAACC,KAAK,wBAAwBC,OAAQ,kBAChC,yBAAKjC,UAAY,oCAAjB,IAAqD,kBAAC,EAAD,MAArD,QAGlB,kBAAC,IAAD,CAAO+B,OAAK,EAACC,KAAK,cAAcC,OAAQ,kBAE9B,yBAAKjC,UAAY,oBAC1B,4BAAQE,GAAK,gBAAgBG,IAAI,0KAA0K6B,MAAM,MAAMC,OAAO,MAAMC,YAAY,iB,GA3EpO7B,IAAMC,WAsFVqB,cAEfQ,IAASJ,OAAQ,kBAAC,EAAD,MAASrC,SAASwB,eAAe,W","file":"static/js/main.ea558137.chunk.js","sourcesContent":["import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport PropTypes from \"prop-types\";\nimport ReactPlayer from \"react-player\";\nimport {render} from \"react-dom\";\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport {HashRouter, Link, Switch} from \"react-router-dom\";\nimport '../portfolio.css'; \nimport './../portfolio.css';\nimport './../index.css';\n\nexport default class ROS_Navigation_Stack_Description extends React.Component {\n       constructor(props) {\n       \t\tsuper(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n        }\n\t\n\t// Set the browser tab name\n\tcomponentDidMount(){\t   \t\n\t\tdocument.body.style.backgroundImage = null;\t\n\t}\n\t\n\tstate = {}\n   render () {                                   \n      return ( \n\t\t      <div className = \"Deep_RL_Description\">\n\t      \t     \n\t      \t      \t\t<div className = \"Banner\">\n                \t        \t<div className = \"bannerName\"> Peter Jochem\n                        \t</div>\n                  \n\t      \t\t<div className = \"moreInfo\">\n                        \t<Link className = \"myResumeLink\" to =\"/ResumePage\" >\n                                \tResume\n                        \t</Link>\n\n                        \t<Link className = \"moreProjectsLink\" to =\"/projects2\" >\n                                \tMore Projects\n                        \t</Link>\n                        </div> \n\t      \t      </div>\n\n\t      {/*\n\t      <div className=\"HeaderVideoContainer\">\n\t      \t<ReactPlayer url=\"https://www.youtube.com/watch?v=-Q_36Qi5CVc\" className=\"HeaderVideo\" />\n\t      </div>\n\t      */}\n\n\t      <div id=\"paragraph_div\">\n      \t   \n\t     <p> Insert a grid of images/gifs</p>\n\n\t      <h1> Learning to Walk on Soft Ground </h1>\n\t      <p> This project was motivated by the work of Dan Lynch. He studies optimal control algorithms for legged robots on yielding terrain. Most of the work on legged robotics assumes the ground is a rigid body, but nature is full of materials that exhibit more complicated dynamics. Dans algorithms require a model of how a robots feet interact with the ground. I worked with Juntao He to develop discrete element method (DEM) simulations of a robots feet intruding into a bed of granular material. We then trained a neural network to map the state of the foot to the ground reaction forces and torques exerted by the granular material. This allows us to have a model of the ground which can then be used by Dans optimal control algorithms. </p>\n\n\t      <p>\n\t      \tThe second part of the project involved learning about reinforcement learning, succesively implementing more complicated deep reinforcement learning algorithms, and eventually trying to solve Dans problem via RL. I started out reading Sutton and Barto (link) and building intution about reinforcement learning problems. I implemented DeepQ Learning, and Double DeepQ Learning in order to build up to more complicated temporal diffrence learning algorithms. I then implemented Deep Determinisitc Policy Gradients and then TD3. Finally, I tried to get a working implementation of a Policies Modulating Trajectories Generators (link) architecture for our hopping robot problem.   \n\t      </p>\n\n\t      <h1> Learning a Model of the Ground </h1>\n              <p>\n              The <a className=\"TDynamics_Link\" href={'https://li.me.jhu.edu/'}> Terradynamics lab </a> at Georgia Tech has done a lot of work on how animal and robotic feet interact with granular materials. They studies the terradynamics of robotic locomotion. The lab has done experimental work to show how certain granular materials ground reaction forces change as a function of how the foot is oriented and the direction it is moving through the material. Juntao He and I re-created this experimental setup in a discrete element method (DEM) simulation. We could then generate datasets describing how the ground reaction forces and torques change as the foot intrudes through the material. A visualization of what the experimental setup is below.  \n\t      </p>\n\t\t\n\t      <div class=\"row\">\n                <div class=\"column_for_intrusion_gif\">\n                        <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/media/intrusion.gif\" alt=\"Foot Intruding into the Granular Material\" alt=\"Foot Intruding Into The Granular Material\"/>\n        </div>\n</div>\n\n\n              <p> We used the DEM simulation of the Terradynamics lab experiments to generate a dataset of foots states and the corresponding ground reaction forces and torques as the foot intrudes into the granular material. We want to predict the ground reaction forces and torques as a function of the foots state. Specefically, we want to map the foots angle of attack, orientation angle, depth, x-velocity, z-velocity, and angular velocity to the ground reaction forces and torques that the foot experiences. Other approaches to this include <a className=\"RFT_Link\" href={'https://li.me.jhu.edu/first-terradynamics-resistive-force-theory/'}> resistive force theory (RFT) </a> but even this very sophisticated method does not utilize the foots velocity information. Neural networks offer a computationally tractable way to learn this function taking into account information that RFT is not suited to use.\n              </p>\n\t<p>\n\t\tWe used our DEM dataset to train a neural network to map the foots state to the corresponding ground reaction forces and torques that the ground exerts on the foot. Below are our comparisons of the neural networks predictions to the ground truth DEM data and the RFT calculations. The curves below are from a single run of the foot interacting with the granular material. At each time step, we recorded the foots state along with ground reaction forces and torques. The yellow curve below are the forces and torques as the DEM simulation calcuated them. In blue are the RFT predictions of what the forces and torques should be given as input the current DEM simulations foot state. We plotted in orange the neural network predictions of what the forces and torques should be given the current DEM simulations foot state.  \n\t      </p>\n\n\t\t\n\t<div class=\"row\">\n        \t<div class=\"column\">\n                \t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/validateModel/DEM_RFT_Comparisons/media/velocity_minus_2_results.png\" alt=\"Learned Mapping Compared to DEM and RFT\"/>\n        </div>\n</div>\n\n\t\n\t     \t\t\t\n\n\n\t\t\n\t      <h1> Learning to Walk On Soft Ground </h1>\n\t      <p> In order to test Deep RL algorithms for the robot hopping on soft ground, I built a custom OpenAI gym environment. The OpenAI gym project was started in order to facilitate a common standard or benchmark for comparing RL algorithms. An agent has a set of actions it can take to influence the environments state and accumulate reward. The agent must learn how to take actions which maximize its cumulative reward over time. One of the more simple gym environments is the pendulum environment. It features a simple pendulum that the agent must learn how to invert. At each time step, the agent recieves the pendulums angle and can choose to either exert a unit of positive torque or negative torque. Although the problem can be easily solved with a PID controller, it serves as a simple test to see if your agent is learning. I built my gym in <a className=\"paper_link\" href=\"https://pybullet.org/wordpress/\"> PyBullet </a>, an open source physics engine. It features a hopping robot with an open chain leg. In high level terms, the agents goal is move in the postive x-direction. I found that the exact details of the agents reward function made a huge diffrence in the agents outcome. Roughly speaking, I rewarded the agent for having a larger x-coordinate, a positive x-velocity, and not falling over. I applied both <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> and <a className=\"paper_link\" href=\"https://arxiv.org/pdf/1802.09477.pdf\"> TD3 </a> to the hopping robot and got interesting results! The agent decided to lock its leg joints and use its foot to generate ground reaction forces. Although this is technically a viable policy, it is far from the natural gait we might have hoped for.                        \n\t      </p>\n\n\t      <p>\n\t\t<a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> (DDPG) is one of the state of the art RL algorithms for continous action spaces. Most of the work in RL has focused on agents who have a discrete number of actions they can take. Some of these discrete action space methods include <a className=\"paper_link\" href=\"https://en.wikipedia.org/wiki/Q-learning\"> Q-Learning </a>, <a className=\"paper_link\" href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\"> DeepQ Networks </a>, and <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.06461\"> Double DeepQ Learning </a>. In our case though, we want to learn how to apply torques to each of the robots motors. We could discretize the set of allowable motor torques but in practice, this does not work well. Instead, we need to use an algorithm designed for continous action spaces. I implemented both DDPG and TD3 for the hopping robot. In order to facilitate testing of DDPG and TD3, I also tried them on the Cheetah and Hopper Environments published by the OpenAI team. Both feature a legged robot that must learn how to locomote by applying a set of motor torques on its joints. This helped me validate the RL algorithms and was also a lot of fun. Below are some of the gaits that the agents learned.         \n\t      </p>\n\n<div class=\"row\">\n \t<div class=\"column\">\n   \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy.gif\" alt=\"Learned Gaits in Gym Environments\"/>\n\t</div>\n\t<div class=\"column\">\n  \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/cheetah2.gif\" alt=\"Learned Gait in OpenAI Gym\" className=\"OpenAI_gym_results_gif2\"/>\n \t</div>\n</div>\n\n\t      <img src=\" https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/h3pper_DDPG2.gif\" alt=\"AI Gym Hopping Robot\" className=\"intrusion_gif\" />\n\t\t\n\t\t</div>\n\t   </div>\t      \n      )\n   }\n}\n\n// Type Check properties here\n//Deep_RL_Description.propTypes = {\n//};\n//Deep_RL_Description.defaultProps = {\n//};\n\n\n","import React from \"react\";\nimport { render } from \"react-dom\";\nimport ReactDOM from 'react-dom';\nimport './portfolio.css'; // Tell webpack that Button.js uses these styles\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport PropTypes from \"prop-types\";\n\n\nclass GridOfPosts extends React.Component {\n        // Set the browser tab name\n        componentDidMount(){\n\n        // document.body.style.backgroundImage = \"url()\";\n        //document.body.style.backgroundImage = null;\n\n        }\n\n        state = {\n        }\n\n   render () {\n      return (\n\t\n\t      <div className = \"GridOfPosts\"> \n\t      \n              <div className=\"grid-container\">\n\t   \t\n\t      {this.props.pageNumber.projects.map( (project) => {\n                        return  <div className = \"box\">  {project.name}\n\t\t\t<a href = {project.url} >\n               <div className=\"grid-item\" id = {project.name} >  <Post name={project.name}  im_url = {project.im_url}  /> \t\t</div> </a>             \n\t\t\t </div>\n                }\n                )}\n\t      </div>   \t \n\n\t      </div>\n\n      )\n   }\n}\n\nGridOfPosts.propTypes = {\n        pageNumber: PropTypes.node.isRequired,\n};\n\nGridOfPosts.defaultProps = {\n\tpageNumber: null,\n};\n\n\nclass Post extends React.Component {\n\tconstructor(props) {\n   \t\t super(props);\t\n\n\t\tthis.componentDidMount = this.componentDidMount.bind(this);\n        \n\t}\n\n        componentDidMount(){\n\n\t\tvar myElement = document.getElementById(this.props.name);\t\n\t\t\n\t\tmyElement.style.backgroundImage = \"url(\" + this.props.im_url + \")\";\n        }\n\n        state = {\n\t\t// This is the number of posts in the grid\n\t\t//numPosts: 9\n\t\t//name: \"\"\n\t}\n\n   render () {\n      return (\n        <div className = \"Post\">\n\n            </div>\n      )\n   }\n}\n\n// Type check the props\nPost.propTypes = {\n\t// No built in image type, image: React.PropTypes.image\n\t// Alternative would be to store the URL/file location of the image\n\tname: PropTypes.string.isRequired,\n\tim_url: PropTypes.string.isRequired\n};\n\nPost.defaultProps = {\n\tname: 'me',\n\tim_url: ''\n};\n\n\nexport default class Portfolio extends React.Component {\n       constructor(props) {\n       \t\tsuper(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n        }\n\n\t\n\t// Set the browser tab name\n\tcomponentDidMount(){\n\t   \t\n\t\t// document.body.style.backgroundImage = \"url()\";\n\t\tdocument.body.style.backgroundImage = null;\t\n\t}\n\t\n\tstate = { \n\t}\n   render () {                                   \n      return (\n\t \n\t      <div>\n\n\t      <div className = \"portfolio\"> \n\t      \t\t<GridOfPosts pageNumber={this.props.pageNumber} />\t\n\t      </div>\n\t      </div>\n      )\n   }\n}\n\n// Type check the props\nPortfolio.propTypes = {\n\tpageNumber: PropTypes.element.isRequired,\n};\n\nPortfolio.defaultProps = {\n\t/* This describes if we are using the first or second page\n\tpageNumber: 1 */\n};\n\n\n","import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport PropTypes from \"prop-types\";\nimport ReactPlayer from \"react-player\";\nimport {render} from \"react-dom\";\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport {HashRouter, Link, Switch} from \"react-router-dom\";\nimport '../portfolio.css'; \nimport './../portfolio.css';\nimport './../index.css';\n\nexport default class Deep_RL_Description extends React.Component {\n       constructor(props) {\n       \t\tsuper(props);\n                this.componentDidMount = this.componentDidMount.bind(this);\n        }\n\t\n\tcomponentDidMount(){\t   \t\n\t\tdocument.body.style.backgroundImage = null;\t\n\t}\n\t\n\tstate = { \n\t}\n   render () {                                   \n      return ( \n\t\t      <div className = \"Deep_RL_Description\">\n\t      \t     \n\t      \t      \t\t<div className = \"Banner\">\n                \t        \t<div className = \"bannerName\"> Peter Jochem\n                        \t</div>\n                  \n\t      \t\t<div className = \"moreInfo\">\n                        \t<Link className = \"myResumeLink\" to =\"/ResumePage\" >\n                                \tResume\n                        \t</Link>\n\n                        \t<Link className = \"moreProjectsLink\" to =\"/projects2\" >\n                                \tMore Projects\n                        \t</Link>\n                        </div> \n\t      \t      </div>\n\n\t      \n\t     <ReactPlayer url=\"https://youtu.be/j5IvaLyz6Rs\" controls=\"True\" className=\"HeaderVideo\" />\n\t      \n\t     <div id=\"paragraph_div\">\n      \t   \n\t      <h1> Learning to Walk on Soft Ground </h1>\n\t      <p> This project was motivated by the work of Dan Lynch. He studies optimal control algorithms for legged robots on yielding terrain. Most of the work on legged robotics assumes the ground is a rigid body, but nature is full of materials that exhibit more complicated dynamics. Dans algorithms require a model of how a robots feet interact with the ground. I worked with Juntao He to develop discrete element method (DEM) simulations of a robots feet intruding into a bed of granular material. We then trained a neural network to map the state of the foot to the ground reaction forces and torques exerted by the granular material. This allows us to have a model of the ground which can then be used by Dans optimal control algorithms. </p>\n\n\t      <p>\n\n\t      \tThe second part of the project involved learning about reinforcement learning, succesively implementing more complicated deep reinforcement learning algorithms, and eventually trying to solve Dans problem via RL. I started out reading Sutton and Barto (link) and building intution about reinforcement learning problems. I implemented <a className=\"paper_link\" href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\"> DeepQ Learning </a>, and <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.06461\"> Double DeepQ Learning </a> in order to build up to more complicated temporal diffrence learning algorithms. I then implemented <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> and then <a className=\"paper_link\" href=\"https://arxiv.org/pdf/1802.09477.pdf\"> TD3 </a>. Finally, I tried to get a working implementation of a <a className=\"paper_link\" href=\"https://arxiv.org/abs/1910.02812\"> Policies Modulating Trajectories Generators </a> architecture for our hopping robot problem.   \n\t      </p>\n\n\t      <h1> Learning a Model of the Ground </h1>\n              <p>\n              The <a className=\"TDynamics_Link\" href={'https://li.me.jhu.edu/'}> Terradynamics lab </a> at Georgia Tech has done a lot of work on how animal and robotic feet interact with granular materials. They studies the terradynamics of robotic locomotion. The lab has done experimental work to show how certain granular materials ground reaction forces change as a function of how the foot is oriented and the direction it is moving through the material. Juntao He and I re-created this experimental setup in a discrete element method (DEM) simulation. We could then generate datasets describing how the ground reaction forces and torques change as the foot intrudes through the material. A visualization of what the experimental setup is below.  \n\t      </p>\n\t\t\n\t      <div class=\"row\">\n                <div class=\"column_for_intrusion_gif\">\n                        <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/media/intrusion.gif\" alt=\"Foot Intruding into the Granular Material\" alt=\"Foot Intruding Into The Granular Material\"/>\n        </div>\n</div>\n\n\n              <p> We used the DEM simulation of the Terradynamics lab experiments to generate a dataset of foots states and the corresponding ground reaction forces and torques as the foot intrudes into the granular material. We want to predict the ground reaction forces and torques as a function of the foots state. Specefically, we want to map the foots angle of attack, orientation angle, depth, x-velocity, z-velocity, and angular velocity to the ground reaction forces and torques that the foot experiences. Other approaches to this include <a className=\"RFT_Link\" href={'https://li.me.jhu.edu/first-terradynamics-resistive-force-theory/'}> resistive force theory (RFT) </a> but even this very sophisticated method does not utilize the foots velocity information. Neural networks offer a computationally tractable way to learn this function taking into account information that RFT is not suited to use.\n              </p>\n\t<p>\n\t\tWe used our DEM dataset to train a neural network to map the foots state to the corresponding ground reaction forces and torques that the ground exerts on the foot. Below are our comparisons of the neural networks predictions to the ground truth DEM data and the RFT calculations. The curves below are from a single run of the foot interacting with the granular material. At each time step, we recorded the foots state along with ground reaction forces and torques. The yellow curve below are the forces and torques as the DEM simulation calcuated them. In blue are the RFT predictions of what the forces and torques should be given as input the current DEM simulations foot state. We plotted in orange the neural network predictions of what the forces and torques should be given the current DEM simulations foot state.  \n\t      </p>\n\n\t\t\n\t<div class=\"row\">\n        \t<div class=\"column\">\n                \t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/h3pper/createGroundModel/validateModel/DEM_RFT_Comparisons/media/velocity_minus_2_results.png\" alt=\"Learned Mapping Compared to DEM and RFT\"/>\n        </div>\n</div>\n\n\t\t\t\n\t      <h1> Learning to Walk On Soft Ground </h1>\n\t      <p> In order to test Deep RL algorithms for the robot hopping on soft ground, I built a custom OpenAI gym environment. The OpenAI gym project was started in order to facilitate a common standard or benchmark for comparing RL algorithms. An agent has a set of actions it can take to influence the environments state and accumulate reward. The agent must learn how to take actions which maximize its cumulative reward over time. One of the more simple gym environments is the pendulum environment. It features a simple pendulum that the agent must learn how to invert. At each time step, the agent recieves the pendulums angle and can choose to either exert a unit of positive torque or negative torque. Although the problem can be easily solved with a PID controller, it serves as a simple test to see if your agent is learning. I built my gym in <a className=\"paper_link\" href=\"https://pybullet.org/wordpress/\"> PyBullet </a>, an open source physics engine. It features a hopping robot with an open chain leg. What makes my environment diffrent is that I use my model of the soft ground to govern the ground reaction forces and torques on the robots foot. In PyBullet, I track where the foot is, and when it is in contact with the granular material, I apply the ground reaction forces and torques that the model predicts. This allows me to have a highly tractable and realistic simulation of soft ground. Below is an image of what the environments looks like.\n</p>\n\t\n\t\n\t<p>\n\t      In high level terms, the agents goal is move in the postive x-direction. I found that the exact details of the agents reward function made a huge diffrence in the agents outcome. Roughly speaking, I rewarded the agent for having a larger x-coordinate, a positive x-velocity, and not falling over. I applied both <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> and <a className=\"paper_link\" href=\"https://arxiv.org/pdf/1802.09477.pdf\"> TD3 </a> to the hopping robot and got interesting results! The agent decided to lock its leg joints and use its foot to generate ground reaction forces. Although this is technically a viable policy, it is far from the natural gait we might have hoped for.                        \n\t</p>\n\n\t      <p>\n\t\t<a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.02971\"> Deep Deterministic Policy Gradients </a> (DDPG) is one of the state of the art RL algorithms for continous action spaces. Most of the work in RL has focused on agents who have a discrete number of actions they can take. Some of these discrete action space methods include <a className=\"paper_link\" href=\"https://en.wikipedia.org/wiki/Q-learning\"> Q-Learning </a>, <a className=\"paper_link\" href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\"> DeepQ Networks </a>, and <a className=\"paper_link\" href=\"https://arxiv.org/abs/1509.06461\"> Double DeepQ Learning </a>. In our case though, we want to learn how to apply torques to each of the robots motors. We could discretize the set of allowable motor torques but in practice, this does not work well. Instead, we need to use an algorithm designed for continous action spaces. I implemented both DDPG and TD3 for the hopping robot. In order to facilitate testing of DDPG and TD3, I also tried them on the Cheetah and Hopper Environments published by the OpenAI team. Both feature a legged robot that must learn how to locomote by applying a set of motor torques on its joints. This helped me validate the RL algorithms and was also a lot of fun. Below are some of the gaits that the agents learned.         \n\t      </p>\n\n<div class=\"row\">\n \t<div class=\"column\">\n   \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy.gif\" alt=\"Learned Gaits in Gym Environments\"/>\n\t</div>\n\t<div class=\"column\">\n  \t\t<img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/cheetah2.gif\" alt=\"Learned Gait in OpenAI Gym\" className=\"OpenAI_gym_results_gif2\"/>\n \t</div>\n</div> \n\n<p className=\"p_describe_pyBullet_ddpg\">\n\tOnce I had validated that my implementation of DDPG worked using the OpenAI teams environments, I applied the algorithm to the new environment in PyBullet. My custom       \n</p>\n\n\n<div class=\"row\">\n\t<div class=\"column_for_pybullet_gif\">\n                        <img src=\"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/h3pper_DDPG2.gif\" alt=\"Custom PyBullet Hopping Robot\"/>\n        </div>\n</div>\t\n\n</div>\n</div>\n      )\n   }\n}\n\n// Type Check properties here\n//Deep_RL_Description.propTypes = {\n//};\n//Deep_RL_Description.defaultProps = {\n//};\n\n\n","import React from \"react\";\nimport { render } from \"react-dom\";\nimport ReactDOM from 'react-dom';\nimport {BrowserRouter, Route} from 'react-router-dom';\nimport Site from \"./index.js\";\n\nclass App extends React.Component {\n    constructor(){\n    super();\n    this.state = {\n     results: {}\n    }\n    }\n \n   render () {\n\treturn (\n\t\t<div className=\"App\">\n\t\t<div className=\"Site\">\n\t\t<Site />\n\t\t</div>\n\t\t</div>\n\t)\n   }\n}\nexport default App;\n// ReactDOM.render(<BrowserRouter basename={process.env.PUBLIC_URL}>< App /></BrowserRouter>, document.getElementById('root'));\n","import React from \"react\";\nimport ReactDOM from 'react-dom';\nimport {render} from \"react-dom\";\nimport {HashRouter, Route, Link, Switch} from \"react-router-dom\";\nimport './index.css'; \nimport './DescriptionPages/Deep_RL_Description.css'\nimport './DescriptionPages/ROS_Navigation_Stack_Description'\n// import './DescriptionPages/Deep_RL_Description.css'\nimport './portfolio.css';\nimport Portfolio from \"./portfolio\";\nimport Deep_RL_Description from \"./DescriptionPages/Deep_RL_Description\"\nimport ROS_Navigation_Stack_Description from \"./DescriptionPages/ROS_Navigation_Stack_Description\"\nimport App from \"./App.js\";\n\nconst page1 = {\n  projects: [\n    \n    {\n        name: \"ROS Navigation\",\n        url: \"https://github.com/PeterJochem/Turtlebot_Navigation\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Turtlebot_Navigation/master/images/tbot_pentagon.gif\"\n    },\n    {\n        name: \"Deep RL\",\n        url: \"https://peterjochem.github.io/Portfolio#/Deep_RL\",\n\tgithub_url: \"https://github.com/PeterJochem/Deep_RL\",\n\tim_url: \"https://raw.githubusercontent.com/PeterJochem/Deep_RL/master/DDPG/media/hopper_learned_policy_cropped.gif\"\n    },\n    {\n        name: \"Mobile Manipulator\",\n        url: \"https://github.com/PeterJochem/Mobile_Manipulator\",\n        im_url: \"https://peterjochem.github.io/myImages/kuka.gif\"\n    },\n    {\n        name: \"Terminator\",\n        url: \"https://github.com/ME495-EmbeddedSystems/final-project-terminator\",\n        im_url: \"https://peterjochem.github.io/myImages/terminator.png\"\n     },\n     {\n        name: \"Chess AI\",\n        url: \"https://github.com/PeterJochem/Chess_AI\",\n        im_url: \"https://peterjochem.github.io/myImages/chess.gif\"\n    }, \n    {\n        name: \"Sawyer Ping Pong\",\n        url: \"https://github.com/PeterJochem/Sawyer_Ping_Pong\",\n        im_url: \"https://peterjochem.github.io/myImages/ping_trajectory.png\"\n     },\n     {\n        name: \"DeepQ Learning\",\n        url: \"https://github.com/PeterJochem/Grid_World_RL\",\n        im_url: \"https://peterjochem.github.io/myImages/NN_Large.png\"\n    },\n    {\n       \tname: \"Triple Pendulum\",\n        url: \"https://github.com/PeterJochem/TriplePendulum\",\n        im_url: \"https://peterjochem.github.io/myImages/pend.gif\"\n    },\n]\n}\n\nconst page2 = {\n  projects: [\n    {\n        name: \"RRT\",\n        url: \"https://github.com/PeterJochem/RRT\",\n        im_url: \"https://peterjochem.github.io/myImages/all_Points_3.png\"\n    },\n     {\n        name: \"CBirch 97\",\n        url: \"https://github.com/PeterJochem/CBirch_97\",\n        im_url: \"https://peterjochem.github.io/myImages/CBirch97.gif\"\n    },\n    {\n        name: \"Canny Edge Detector\",\n        url: \"https://github.com/PeterJochem/CannyEdgeDetector\",\n        im_url: \"https://peterjochem.github.io/myImages/Lena_Processed.png\"\n    },\n    {\n        name: \"GAN\",\n        url: \"https://github.com/PeterJochem/MNIST_GAN\",\n        im_url: \"https://raw.githubusercontent.com/PeterJochem/Portfolio/master/src/images/GAN.gif\"\n    },\n    {\n        name: \"Neural Network Snake\",\n        url: \"https://github.com/PeterJochem/Snake\",\n        im_url: \"https://peterjochem.github.io/myImages/Snake.png\"\n    }\n  ]\n}\n\n\nclass Site extends React.Component {\n\ncomponentDidMount(){\n\t// Set the browser tab name\n\tdocument.title = \"Peter Jochem\"\n}\n\nconstructor(props) {\n\tsuper(props);\n}\n\n  render() {\n    return (\n\t    <HashRouter>\n\t    <Switch>\n       <div>\n          \n\t    <Route exact path='/' render={() => (\n            \t\n\t\t<div>\n\t\t\t<div className = \"Banner\"> \t\n\t\t  \t<div className = \"bannerName\"> Peter Jochem \n\t\t  \t</div>\n\t\t   \t<div className = \"moreInfo\">\n\t\t  \t\n\t\t\t<Link className = \"myResumeLink\" to =\"/ResumePage\" >\n                                Resume\n                        </Link>\n\n\t\t  \t<Link className = \"moreProjectsLink\" to =\"/projects2\" >  \n\t\t    \t\tMore Projects\n\t\t    \t</Link>\n\t\t        </div>\t\n\t\t</div>\n\n\t\t<div className = \"portfolio\"> <Portfolio  pageNumber = {page1} />  </div>\t\n\t\t</div>\t\n\n    \t\t)} />\n\t\t\n\t     <Route exact path='/projects2' render={() => (\n                <div>\n\t\t\t<div className = \"Banner\">\n                        <div className = \"bannerName\"> Peter Jochem\n                        </div>\n                        <div className = \"moreInfo\">\n\n\t\t     \t<Link className = \"myResumeLink\" to =\"/ResumePage\" >\n                        \tResume\n\t\t\t</Link>\n\n\t\t     \t<Link className = \"moreProjectsLink\" to =\"/\" >\n                                More Projects\n                        </Link>\n\t\t     \t\n                        </div>\n                        <div className = \"BannerImage\"> </div>\n                </div>\n                \n\t\t<div className = \"portfolio\"> <Portfolio  pageNumber = {page2} />  </div>\t\t\n\t\t</div>\n\n                )} />\n\n\t    \t<Route exact path='/Deep_RL' render={() => (\n                        <div className = \"Deep_RL_Description\"> <Deep_RL_Description /> </div>\n                )} />\n\n\t    \t<Route exact path='/ROS_Navigation_Stack' render={() => (\n                        <div className = \"ROS_Navigation_Stack_Description\"> <ROS_Navigation_Stack_Description /> </div>\n                )} />\n\n\t    \t<Route exact path='/ResumePage' render={() => (\n\n                <div className = \"resumeBackground\">\n\t\t    \t<iframe id = \"myResumeFrame\" src=\"http://docs.google.com/viewer?url=https://raw.githubusercontent.com/PeterJochem/PeterJochem.github.io/6331c077619a0b5593287d0fcead9a932102b569/Resume.pdf&embedded=true\" width=\"600\" height=\"780\" frameborder=\"0\"></iframe>\n\t\t    </div>\n                )} />\n\t  \n\t    \n\t  </div>\n\t  </Switch>\n\t  </HashRouter>\n    );\n  }\n}\nexport default Site;\n\nReactDOM.render( <App />, document.getElementById('root') );\n// ReactDOM.render(<BrowserRouter basename={process.env.PUBLIC_URL}>< App /></BrowserRouter>, document.getElementById(\"root\"));\n\n"],"sourceRoot":""}